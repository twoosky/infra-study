Controller
===
# Controller 기능
* 쿠버네티스의 Controller는 서비스를 관리하고 운영하는데 큰 도움을 준다.
* Auto Healing
  * 장애가 난 서버(Node) 위에 있는 서비스들을 다른 서버(Node)로 자동으로 옮겨주는 기능을 지원한다.  
  * `Node` 위에 있는 `Pod`가 다운되거나 `Pod`가 존재하는 `Node`가 다운되는 경우 `Pod`에서 실행되던 서비스에 장애 발생한다.    
  * 이때 `Controller`는 이를 바로 인지하고 *다른 Node에 Pod를 즉각적으로 다시 생성*해준다.  
* Auto Scaling
  * `Pod`의 리소스가 한계에 도달한 경우 `Controller`는 자동으로 상태를 파악해 `Pod`를 하나 더 생성해줌으로써 부하를 분산시키고, `Pod`가 죽지 않도록 한다.
  * 이를 통해 서비스는 성능에 대한 장애 없이 안정적인 서비스 운영 가능하다.  
* Software Update
  * 여러 `Pod`에 대한 버전을 업그레이드 해야되는 경우 `Controller`를 통해 한 번에 업그레이드 가능하고, 업그레이드 도중 문제 발생 시 롤백할 수 있는 기능을 제공한다.
* Job
  * 일시적인 작업을 해야되는 경우 `Controller`가 필요한 순간에만 `Pod`를 만들어서 해당 작업을 수행하고 작업이 완료되면 삭제한다.  
  * 그 순간에만 자원이 사용되고, 작업 후에 다시 반환되기 때문에 효율적인 자원 활용이 가능해진다.
# Replication Controller, ReplicaSet
Replication Controller는 현재 더이상 사용되지 않는 오브젝트이고, 이를 대체하는`ReplicaSet`를 사용하자.
* ## Template
  * `Controller`와 `Pod`는 `Service`와 `Pod`처럼 `label`과 `selector`로 연결된다. 
    * `type:web` lavel이 붙어있는 `Pod`가 있고,  `Controller`에 `type:web` selector를 주면 해당 Pod와 Controller는 연결된다.
  * `Controller`를 만들 때  `templete`에 `Pod`의 내용을 넣는다. 
  * `Controller`에는 `Auto Heling`기능이 있어 `Pod`가 죽는 경우 `Pod`를 재생성하는데, 이때 `templete`의 내용으로 `Pod`를 재생성 한다.
  * 위의 특성을 사용해 앱에 대한 업그레이드를 할 수 있다.
    1. `templete`에 v2에 대한 `Pod`를 업데이트한다. [Pod:v2]
    2. 기존에 연결되어 있던 `Pod`를 다운시킨다.
    3. `Controller`는 `templete`의 내용으로 `Pod`를 재생성한다. 
    * 따라서 `templete`에 새로 업데이트한 `Pod:v2`의 내용으로 `Pod`가 재생성되면서 버전 업그레이드를 수동으로 할 수 있다.
  * Pod yaml 파일
  ```yml
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-1
    labels:
      type: web
    spec:
      containers:
      - name: container
        image: tmkube/app:v1
  ```
  * Controller yaml 파일
  ```yml
  apiVersion: v1
  kind: ReplicationController
  metadata:
    name: replication-1
  spec:
    replicas: 1
    # selector를 통해 Pod와 연결
    selector:
      type: web
    # template에 Pod의 내용 정의
    template:
      metadata:
        name: pod-1
        # label을 지정해 추후 재생성된 Pod와 Controller와 연결
        labels:
          type: web
      spec:
        containers:
          - name: container
            image: tmkube/app:v2
  ```
* ## Replicas
  * `replicas` 수만큼 `Pod`의 개수가 관리된다.
  * `Pod`가 삭제되면 `replicas`수에 맞게 `Pod`를 재생성한다.
  * `replicas` 수에 따라 scale-out, scale-in을 할 수 있다.
    * scale-out: replicas 수만큼 pod 개수 확장
      * replicas가 1 -> 3 으로 변경되었다면, pod를 3개로 확장
    * scale-in: replicas 수만큼 pod 개수 축소
  * `Pod`와 `Controller`를 따로 만들어 연결하지 않고, 한 번에 만들 수 있다.
    1. `replicas`와 `templete`를 포함한 `Controller`를 만든다.
    2. `Controller`에 연결되어 있는 `Pod`가 없기 때문에 `Controller`에서 `templete` 내용으로 `replicas` 수 만큼 `Pod`를 생성한다. 
    * 실제로 `Controller`를 사용할 때 `Controller`에 대한 내용만 만들어 사용한다.
* ## Selector
  * `ReplicaSet`에만 있는 기능
    * `Template`, `Replicas` 기능은 Replication Controller와 ReplicaSet에 모두 존재
  * `Replication Controller`의 `Selector`
    *  key와 value가 모두 같은 `label`을 가진 `Pod`들만 연결한다. key, vlaue 중 하나라도 다른 `Pod`는 연결하지 않는다.
    * `type:web` selector를 가진 Replication Controller는 `type:web` label을 가진 Pod와만 연결하고, `type:db' label을 가진 Pod와는 연결하지 않는다.
  * `ReplicaSet`의 `Selector`
    * matchLabels, matchExpressions 속성이 있다.
    * matchLabels
      * Replication Controller와 같이 key, value가 모두 같은 `label`을 가진 `Pod`와만 연결한다.
    * matchExpressions
      * 선택할 key, value를 좀 더 섬세하게 지정할 수 있다.
        * ex) {key:var, operator: Exists}를 selector로 주는 경우 value에 상관없이 key값이 var인 `label`을 가진 `Pod`들과 연결한다.
      * operator 옵션
        * Exists: value값에 상관없이 key값이 일치하는 `label`을 가진 `Pod`들과 연결
        * DoesNotExist: Exists와 반대로 key값이 일치하지 않는 `label`을 가진 `Pod`들과 연결
          * Controller의 selector가 key:A 인 경우 key값이 A가 아닌 label을 가진 Pod들만 연결 
        * In: value로 가능한 여러 후보들을 지정할 수 있다.
          * Controller의 selector를 {key:A, operator: In, values: {2,3}}으로 설정한 경우 key값이 A이고, value값이 2 또는 3인 `label`을 가진 `Pod`들과 연결
        * NotIn: In과 반대로 values값을 포함하지 않는 `label`을 가진 `Pod`들과 연결
        * <img src=""> 아님 selector 이미지랑 operator 옵션 이미지 이어서 위에 한번에 올리던지
    *  yaml 파일
    ```
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: replica-1
    spec:
      replicas: 3
      selector:
        # key, value가 모두 일치하는 label을 가진 Pod와 연결
        matchLabels:
          type: web
        # operator옵션에 따라 선택할 key, value 지정
        matchExpressions:
          - {key: ver, operator: Exists}
      template:
        metadata:
          name: pod
    ...
    ```
# Deployment - Recreate, RollingUpdate
* `Deployment`는 현재 한 서비스가 운영 중인데, 이 서비스를 업데이트해 재배포를 해야할 때 도움을 주는 `Controller`이다.
* K8S의 서비스 업그레이드 방식
<img src="">

  * ReCreate
    * 기존 `Pod`를 삭제하고 업그레이드 된 `Pod`생성
    * `Pod`를 삭제하고 재생성하기까지 Downtime이 발생하므로 일시적인 정지가 가능한 서비스인 경우에만 `ReCreate`방식을 사용할 수 있다.
  * Rolling Update
    * 업그레이드된 `Pod`를 생성한 후 기존 `Pod`삭제
    * 기존의 `Pod`와 새로운 `Pod`가 모두 존재하는 동안 추가적인 자원을 요구하지만, Downtime이 발생하지 않는다. 
  * Blue/Green
    * `Service`의 `selector`를 변경해 업그레이드된 `Pod`와 연결
      * 예시) `Controller`에 `ver:v1` label을 가지는 `Pod` 2개가 있고, `Service`의 selector를 `ver:v1`로 지정해 위 `Pod`들이 `Service`에 연결되어 있다고 가정
      * 새로운 버전을 배포하기 위해 `ver:v2` label을 가진 `Pod` 2개를 만들고, 이를 `Controller`에 연결한다.
      * `Service`의 selector를 `ver:v2`로 변경하여 새로운 버전의 `Pod`들과 연결해 서비스를 업그레이드하는 방식이다.
    * 순간적으로 변경되므로 `Service`에 대한 Downtime은 발생하지 않는다.
    * `ver:v2`에 문제가 발생한 경우 `Service`의 selector만 `ver:v1`으로 변경하면 되므로 쉽게 롤백할 수 있다는 장점이 있다.
    * `ver:v2`에 문제가 없는 경우 기존 버전의 `Pod`는 삭제하면 된다.
    * `ReCreate`방식과 같이 추가적인 자원을 요구한다는 단점이 있지만, 상당히 많이 사용하고 안정적인 배포 방식이다. 
  * Canary
    * 불특정 다수에 대한 테스팅
      1. `ver:v1`과 `ty:app` label을 가지는 `Pod`가 있고, `ty:app`label을 통해 `Service`와 연결한다.
      2. 그 후 버전 테스트용 `Controller`를 만들 때 `replica`값을 설정해 해당 수만큼 `ver:v2`, `ty:app` label을 가진 `Pod`들을 생성한다. 
      3. `ver:v2`의 `Pod`들을 `ty:app`label을 통해 `Service`와 연결한다.
      4. 이렇게 하면 `Service`로 들어오는 트래픽 중 일부는 `ver:v2`의 `Pod`로 들어가고, 일부는 `ver:v1`의 `Pod`로 들어가게 된다. 
      5. 문제 발생시 `ver:v2` `Controller`의 `replica` 수만 0으로 설정하면 된다. 
    * 특정 target에 대한 테스팅
      1. 버전에 따라 각각의 서비스를 생성한다.
      2. `Ingress Controller`을 사용해 유입된 트래픽을 `url path`에 따라서 각 `Service`에 전달한다.
        * ex) `/app` path를 통해 들어온 트래픽은 `ver:v1` Service에 전달, `/v2/app` path를 통해 들어온 트래픽은 `ver:v2` Service에 전달하는 방식
        * ex) 미국에서 접근하면 url 앞에 `/In`을 붙이도록 설정해 미국에서 접근하는 사용자에게만 `ver:v2`에 대한 `Service`로 연결한다. 이처럼 특정 target을 정해놓고 테스트할 수 있다.
      3. 테스트 기간이 종료되고 문제가 없으면 `ver:v2` Controller의 `replica` 수를 증가시키고, `Ingress Controller`의 설정을 변경한 후에 기존 `ver:v1`에 대한 내용을 삭제한다.
    * Downtime이 발생하지 않지만, 추가적인 자원을 요구한다.
* ## ReCreate
<img src="ReCreate1">

  * Service 배포 과정
    1. ReplicaSet과 같이 `selector`,`replicas`,`template`값을 넣어 `Deployment`를 생성한다.
      * 위 값들은 `Deployment`가 직접 `Pod`를 생성해 관리하기 위한 값이 아닌, `ReplicaSet`에 값을 넘겨주기 위함이다.
    2. 위 값들로 `ReplicaSet`을 생성하고, `Pod`를 생성한다.
    3. 마지막으로 `Service`를 생성해 `Pod`들의 label로 연결하면 `Service`를 통해 `Pod`에 접근할 수 있게 된다. 
<img src="ReCreate2">

  * ReCreate로 Service 업그레이드 과정
    1. `Deployment`의 `template`를 v2의 `Pod` 내용으로 변경한다.
    2. `Deployment`는 먼저 ReplicaSet의 `replicas`를 0으로 변경해 `Pod`를 제거한다.
    3. 새로운 `ReplicaSet`을 만들어 변경된 `template`의 내용을 넣고, 업그레이된 버전의 `Pod`들을 생성한다.
    4. 새로운 `Pod`의 lable은 이전 `Pod`와 같기 때문에 `Service`는 자동으로 새로 생성된 `Pod`들과 연결된다.
  * Deployment yaml 파일
  ```yml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: deployment-1
  spec:
    # Deployment 생성 시 selector, replicas, template 정보 필요
    selector:
      matchLabels:
        type: app
    replicas: 2
    strategy:
      # 배포 방식
      type: Recreate
    # 남겨둘 ReplicaSet의 개수
    revisionHistoryLimit: 1
    template:
      metadata:
        labels:
          type: app
      spec:
        containers:
        - name: container
          image: tmkube/app:v1
   ```
   > `revisionHistoryLimit`는 남겨둘 `ReplicaSet`의 개수를 의미한다. (default: 10)
   > `revisionHistoryLimit`값이 1인 경우 업그레이드 할 때 직전 version의 `ReplicaSet`만 남겨두고 이전 `ReplicaSet`은 삭제한다. 즉, replicas가 0인 `ReplicaSet`을 하나만 남긴다는 의미이다.  
   > 0이 된 `ReplicaSet`은 이전 버전으로 돌아가고 싶을 때 사용된다. 
   * Service yaml 파일
   ```yml
   apiVersion: v1
   kind: Service
   metadata:
     name: svc-1
   spec:
     # 연결할 Pod의 label과 동일하게 selector 지정
     selector:
       type: app
     ports:
       - port: 8080
         protocol: TCP
         targetPort: 8080
  ```
* ## Rolling Update
<img src="ReCreate1">

  * 위와 같이 서비스가 운영되고 있다고 하자
<img src="Rolling Update1", "Rolling Update2">
  * Rolling Update로 서비스 업그레이드 과정
    1. `Deployment`의 `template`를 v2의 `Pod` 내용으로 변경한다.
    2. `replicas`값이 1인 `ReplicaSet`을 생성하고, 새로운 `Pod` 1개를 생성한다.
    3. 새로운 `Pod`의 label이 `Service`의 selector와 같으므로 연결된다.
    4. `Service`에서는  트래픽이 v1/v2 `Pod`에 분산되어 보내진다.
    5. 새로운 `Pod`가 생성되면 기존의(v1) `ReplicaSet`의 replicas값을 2에서 1로 변경한다. v1의 `Pod` 중 하나가 삭제된다. 
    6. 새로운(v2) `ReplicaSet`의 replicas를 2로 변경해 새로운 `Pod`를 생성한다. 
    7. 기존의(v1) `ReplicaSet`의 replicas를 0으로 변경해 남은 `Pod`를 모두 삭제한다.
  * ReCreate와 마찬가지로 기존의 `ReplicaSet`을 지우지 않고 배포를 종료하게 된다.
  * Rolling Update yaml 파일
  ```yml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: deployment-2
  spec:
    # selector, replicas, template 설정
    selector:
      matchLabels:
        type: app
    replicas: 2
    strategy:
      # 배포 방식
      type: RollingUpdate
    minReadySeconds: 10
    template:
      metadata:
        labels:
          type: app
      spec:
        containers:
        - name: container
          image: tmkube/app:v1
  ```
  > minReadySeconds는 기존 ReplicaSet의 replicas 수를 줄여 Pod를 삭제하고, 새로운 ReplicaSet에서 Pod를 생성하는 과정에서 대기할 시간  
  * 새로 생성된 ReplicaSet의 selector와 기존에 있던 Pod의 label이 같은데 그럼 둘이 연결되지 않나?
  > Deployment가 ReplicaSet을 만들 때 ReplicaSet의 selector와 Pod의 label만을 사용해 매칭하지 않고, 추가적인 label과 selector를 만들어주기 때문에 새로 생성된 ReplicaSet과 기존의 Pod는 연결되지 않는다.

# DaemonSet, Job, CronJob
* DaemonSet
  * 이전에 `Controller`는 NodeSchedule에 의해 `Node`에 `Pod`를 배치할 때 각 `Node`의 사용 가능한 자원량을 기준으로 비교적 여유로운 `Node`에 우선적으로 `Pod`를 배치했다.
  * 반면 `DaemonSet`은 `Node`의 자원 상태에 상관없이 모든 `Node`에 `Pod`를 하나씩 생성한다. 
  * 만약 `Node`가 10개면 각 `Node`에 하나씩 총 10개의 `Pod`를 생성한다.
  * 위처럼 각각의 `Node`마다 설치해 사용해야하는 서비스 종류
    * Prometheus: 성능 모니터링을 위한 서비스
    * Fluentd: 특정 `Node`에 발생하는 장애를 대비하기 위해 로깅을 담당하는 서비스
    * GlusterFS: `Node`들 내에 Storage로 활용하기 위한 서비스
 * Job
   * `Node1`에 직접 만든 `Pod`, `ReplicaSet`을 통해 만든 `Pod`, `Job`을 통해 만든 `Pod`가 있다고 할 때 `Node1`에 장애가 발생했다고 가정
   * 직접 만든 `Pod`에도 장애가 발생해 해당 서비스는 더이상 유지할 수 없다.
   * `Controller`에 의해 생성된 `Pod`는 장애가 발생하면 ReCreate해 다른 `Node`에 재생성 된다.
   * `ReplicaSet`을 통해 재생성된 `Pod`는 작동하지 않으면 Restart시켜 주기 때문에 해당 `Pod`에 있는 서비스는 어떤 상황에서도 유지된다.
     * ReCreate: `Pod`를 다시 생성하기 때문에 `Pod`의 이름, IP들이 변경된다.
     * ReStart: `Pod` 내 컨테이너만 재기동 시켜준다.
   * `Job`을 통해 재생성된 `Pod`는 작동하지 않으면 종료된다. 
     * `Pod`가 삭제되진 않고, 자원을 사용하지 않는 상태로 머무른다. 
     * `Pod` 안의 log를 통해 작업 결과를 볼 수 있다.
     * 이후 필요가 없으면 직접 `Pod`를 삭제하면 된다.
* CronJob
  * `Job`들을 주기적인 시간에 따라서 생성하는 역할을 한다.
  * `Job`을 여러개 묶어 `CronJob`으로 만들어 `CronJob`단위로 특정 시간에 반복적으로 실행할 목적으로 사용한다. 
  * 주기적인 Backup, Checking, Messaging 등에 사용
