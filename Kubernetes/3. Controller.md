Controller
===
* [Controller 기능](#Controller-기능)
* [Replication Controller, ReplicaSet](#Replication-Controller,-ReplicaSet)
* [Deployment](#Deployment)
* [DaemonSet, Job, CronJob](#DaemonSet,-Job,-CronJob)
# Controller 기능
<img src="https://user-images.githubusercontent.com/50009240/141164869-5f5615b2-a198-4528-973b-2c49f76b988a.png" width="650" height="300">

* 쿠버네티스의 Controller는 서비스를 관리하고 운영하는데 큰 도움을 준다.
* Auto Healing
  * 장애가 난 서버(Node) 위에 있는 서비스들을 다른 서버(Node)로 자동으로 옮겨주는 기능을 지원한다.  
  * `Node` 위에 있는 `Pod`가 다운되거나 `Pod`가 존재하는 `Node`가 다운되는 경우 `Pod`에서 실행되던 서비스에 장애 발생한다.    
  * 이때 `Controller`는 이를 바로 인지하고 *다른 Node에 Pod를 즉각적으로 다시 생성*해준다.  
* Auto Scaling
  * `Pod`의 리소스가 한계에 도달한 경우 `Controller`는 자동으로 상태를 파악해 `Pod`를 하나 더 생성해줌으로써 부하를 분산시키고, `Pod`가 죽지 않도록 한다. 이를 통해 서비스는 성능에 대한 장애 없이 안정적인 서비스 운영 가능하다.  
* Software Update
  * 여러 `Pod`에 대한 버전을 업그레이드 해야되는 경우 `Controller`를 통해 한 번에 업그레이드 가능하고, 업그레이드 도중 문제 발생 시 롤백할 수 있는 기능을 제공한다.
* Job
  * 일시적인 작업을 해야되는 경우 `Controller`가 필요한 순간에만 `Pod`를 만들어서 해당 작업을 수행하고 작업이 완료되면 삭제한다.  
  * 그 순간에만 자원이 사용되고, 작업 후에 다시 반환되기 때문에 효율적인 자원 활용이 가능해진다.
# Replication Controller, ReplicaSet
Replication Controller는 현재 더이상 사용되지 않는 오브젝트이고, 이를 대체하는`ReplicaSet`를 사용하자.

<img src="https://user-images.githubusercontent.com/50009240/141165669-70de4733-a47c-491a-a2fe-d08849ad3ea0.png" width="1000" hight="400">

* ## Template
  * `Controller`와 `Pod`는 `Service`와 `Pod`처럼 `label`과 `selector`로 연결된다. 
    * `type:web` lavel이 붙어있는 `Pod`가 있고,  `Controller`에 `type:web` selector를 주면 해당 Pod와 Controller는 연결된다.
  * `Controller`를 만들 때  `templete`에 `Pod`의 내용을 넣는다. 
  * `Controller`에는 `Auto Heling`기능이 있어 `Pod`가 죽는 경우 `Pod`를 재생성하는데, 이때 `templete`의 내용으로 `Pod`를 재생성 한다.
  * 위의 특성을 사용해 앱에 대한 업그레이드를 할 수 있다.
    1. `templete`에 v2에 대한 `Pod`를 업데이트한다. [Pod:v2]
    2. 기존에 연결되어 있던 `Pod`를 다운시킨다.
    3. `Controller`는 `templete`의 내용으로 `Pod`를 재생성한다. 
    * 따라서 `templete`에 새로 업데이트한 `Pod:v2`의 내용으로 `Pod`가 재생성되면서 버전 업그레이드를 수동으로 할 수 있다.
  * Pod yaml 파일
  ```yml
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-1
    labels:
      type: web
    spec:
      containers:
      - name: container
        image: tmkube/app:v1
  ```
  * Controller yaml 파일
  ```yml
  apiVersion: v1
  kind: ReplicationController
  metadata:
    name: replication-1
  spec:
    replicas: 1
    # selector를 통해 Pod와 연결
    selector:
      type: web
    # template에 Pod의 내용 정의
    template:
      metadata:
        name: pod-1
        # label을 지정해 추후 재생성된 Pod와 Controller와 연결
        labels:
          type: web
      spec:
        containers:
          - name: container
            image: tmkube/app:v2
  ```
* ## Replicas
  * `replicas` 수만큼 `Pod`의 개수가 관리된다.
  * `Pod`가 삭제되면 `replicas`수에 맞게 `Pod`를 재생성한다.
  * `replicas` 수에 따라 scale-out, scale-in을 할 수 있다.
    * scale-out: replicas 수만큼 pod 개수 확장
      * replicas가 1 -> 3 으로 변경되었다면, pod를 3개로 확장
    * scale-in: replicas 수만큼 pod 개수 축소
  * `Pod`와 `Controller`를 따로 만들어 연결하지 않고, 한 번에 만들 수 있다.
    1. `replicas`와 `templete`를 포함한 `Controller`를 만든다.
    2. `Controller`에 연결되어 있는 `Pod`가 없기 때문에 `Controller`에서 `templete` 내용으로 `replicas` 수 만큼 `Pod`를 생성한다. 
  * 실제로 `Controller`를 사용할 때 `Controller`에 대한 내용만 만들어 사용한다.
* ## Selector
  * `ReplicaSet`에만 있는 기능
    * `Template`, `Replicas` 기능은 Replication Controller와 ReplicaSet에 모두 존재
  * Replication Controller의 Selector
    *  key와 value가 모두 같은 `label`을 가진 `Pod`들만 연결한다. key, vlaue 중 하나라도 다른 `Pod`는 연결하지 않는다.
    * `type:web` selector를 가진 Replication Controller는 `type:web` label을 가진 Pod와만 연결하고, `type:db' label을 가진 Pod와는 연결하지 않는다.
  * ReplicaSet의 Selector
    * matchLabels, matchExpressions 속성이 있다.
    * `matchLabels`: Replication Controller와 같이 key, value가 모두 같은 `label`을 가진 `Pod`와만 연결한다.
    * `matchExpressions`: 선택할 key, value를 좀 더 섬세하게 지정할 수 있다.
      * {key:var, operator: Exists}를 selector로 주는 경우 value에 상관없이 key값이 var인 `label`을 가진 `Pod`들과 연결한다.
      * operator 옵션  
      * <img src="https://user-images.githubusercontent.com/50009240/141166066-379d61ac-5b75-40b6-9844-eb055f488a49.png" width="350" hight="150">
      * Exists: value값에 상관없이 key값이 일치하는 `label`을 가진 `Pod`들과 연결
      * DoesNotExist: Exists와 반대로 key값이 일치하지 않는 `label`을 가진 `Pod`들과 연결
        * Controller의 selector가 key:A 인 경우 key값이 A가 아닌 label을 가진 Pod들만 연결 
      * In: value로 가능한 여러 후보들을 지정할 수 있다.
        * Controller의 selector를 {key:A, operator: In, values: {2,3}}으로 설정한 경우 key값이 A이고, value값이 2 또는 3인 `label`을 가진 `Pod`들과 연결
      * NotIn: In과 반대로 values값을 포함하지 않는 `label`을 가진 `Pod`들과 연결
    *  yaml 파일
    ```yml
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: replica-1
    spec:
      replicas: 3
      selector:
        # key, value가 모두 일치하는 label을 가진 Pod와 연결
        matchLabels:
          type: web
        # operator옵션에 따라 선택할 key, value 지정
        matchExpressions:
          - {key: ver, operator: Exists}
      template:
        metadata:
          name: pod
    ...
    ```
# Deployment
* `Deployment`는 현재 한 서비스가 운영 중인데, 이 서비스를 업데이트해 재배포를 해야할 때 도움을 주는 `Controller`이다.
<img src="https://user-images.githubusercontent.com/50009240/141168081-4bec28e6-1535-4ef6-9334-e07692a3c460.png" width="800" height="370">

* K8S의 서비스 업그레이드 방식
  * ReCreate
    * 기존 `Pod`를 삭제하고 업그레이드 된 `Pod`생성
    * `Pod`를 삭제하고 재생성하기까지 Downtime이 발생하므로 일시적인 정지가 가능한 서비스인 경우에만 `ReCreate`방식을 사용해 업그레이드할 수 있다.
  * Rolling Update
    * 업그레이드된 `Pod`를 생성한 후 기존 `Pod`삭제
    * 기존의 `Pod`와 새로운 `Pod`가 모두 존재하는 동안 추가적인 자원을 요구하지만, Downtime이 발생하지 않는다. 
  * Blue/Green
    * `Service`의 `selector`를 변경해 업그레이드된 `Pod`와 연결
      * 예시) `Controller`에 `ver:v1` label을 가지는 `Pod` 2개가 있고, `Service`의 selector를 `ver:v1`로 지정해 위 `Pod`들이 `Service`에 연결되어 있다고 가정
      * 새로운 버전을 배포하기 위해 `ver:v2` label을 가진 `Pod` 2개를 만들고, 이를 `Controller`에 연결한다.
      * `Service`의 selector를 `ver:v2`로 변경하여 새로운 버전의 `Pod`들과 연결해 서비스를 업그레이드하는 방식이다.
    * 순간적으로 변경되므로 `Service`에 대한 Downtime은 발생하지 않는다.
    * `ver:v2`에 문제가 발생한 경우 `Service`의 selector만 `ver:v1`으로 변경하면 되므로 쉽게 롤백할 수 있다는 장점이 있다.
    * `ver:v2`에 문제가 없는 경우 기존 버전의 `Pod`는 삭제하면 된다.
    * `ReCreate`방식과 같이 추가적인 자원을 요구한다는 단점이 있지만, 상당히 많이 사용하고 안정적인 배포 방식이다. 
  * Canary
    * 불특정 다수에 대한 테스팅
      1. `ver:v1`과 `ty:app` label을 가지는 `Pod`가 있고, `ty:app`label을 통해 `Service`와 연결한다.
      2. 그 후 버전 테스트용 `Controller`를 만들 때 `replica`값을 설정해 해당 수만큼 `ver:v2`, `ty:app` label을 가진 `Pod`들을 생성한다. 
      3. `ver:v2`의 `Pod`들을 `ty:app`label을 통해 `Service`와 연결한다.
      4. 이렇게 하면 `Service`로 들어오는 트래픽 중 일부는 `ver:v2`의 `Pod`로 들어가고, 일부는 `ver:v1`의 `Pod`로 들어가게 된다. 
      5. 문제 발생시 `ver:v2` `Controller`의 `replica` 수만 0으로 설정하면 된다. 
    * 특정 target에 대한 테스팅
      1. 버전에 따라 각각의 서비스를 생성한다.
      2. `Ingress Controller`을 사용해 유입된 트래픽을 `url path`에 따라서 각 `Service`에 전달한다.
        * ex) `/app` path를 통해 들어온 트래픽은 `ver:v1` Service에 전달, `/v2/app` path를 통해 들어온 트래픽은 `ver:v2` Service에 전달하는 방식
        * ex) 미국에서 접근하면 url 앞에 `/In`을 붙이도록 설정해 미국에서 접근하는 사용자에게만 `ver:v2`에 대한 `Service`로 연결한다. 이처럼 특정 target을 정해놓고 테스트할 수 있다.
      3. 테스트 기간이 종료되고 문제가 없으면 `ver:v2` Controller의 `replica` 수를 증가시키고, `Ingress Controller`의 설정을 변경한 후에 기존 `ver:v1`에 대한 내용을 삭제한다.
    * Downtime이 발생하지 않지만, 추가적인 자원을 요구한다.
* ## ReCreate
<img src="https://user-images.githubusercontent.com/50009240/141169401-b40bbf04-c70e-4acf-a271-34b0d1137829.png" width="500" height="210">

  * Service 배포 과정
  1. ReplicaSet과 같이 `selector`,`replicas`,`template`값을 넣어 `Deployment`를 생성한다.
    * 위 값들은 `Deployment`가 직접 `Pod`를 생성해 관리하기 위한 값이 아닌, `ReplicaSet`에 값을 넘겨주기 위함이다.
  2. 위 값들로 `ReplicaSet`을 생성하고, `Pod`를 생성한다.
  3. 마지막으로 `Service`를 생성해 `Pod`들의 label로 연결하면 `Service`를 통해 `Pod`에 접근할 수 있게 된다. 
<img src="https://user-images.githubusercontent.com/50009240/141169858-80d7b666-aa4f-42bc-9e7f-5835ffe2fb92.png" width="500" height="210">

  * ReCreate로 Service 업그레이드 과정
  1. `Deployment`의 `template`를 v2의 `Pod` 내용으로 변경한다.
  2. `Deployment`는 먼저 ReplicaSet의 `replicas`를 0으로 변경해 `Pod`를 제거한다.
  3. 새로운 `ReplicaSet`을 만들어 변경된 `template`의 내용을 넣고, 업그레이된 버전의 `Pod`들을 생성한다.
  4. 새로운 `Pod`의 lable은 이전 `Pod`와 같기 때문에 `Service`는 자동으로 새로 생성된 `Pod`들과 연결된다.
  * Deployment yaml 파일
  ```yml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: deployment-1
  spec:
    # Deployment 생성 시 selector, replicas, template 정보 필요
    selector:
      matchLabels:
        type: app
    replicas: 2
    strategy:
      # 배포 방식
      type: Recreate
    # 남겨둘 ReplicaSet의 개수
    revisionHistoryLimit: 1
    template:
      metadata:
        labels:
          type: app
      spec:
        containers:
        - name: container
          image: tmkube/app:v1
   ```
   > `revisionHistoryLimit`는 남겨둘 `ReplicaSet`의 개수를 의미한다. (default: 10)  
   > `revisionHistoryLimit`값이 1인 경우 업그레이드 할 때 직전 version의 `ReplicaSet`만 남겨두고 이전 `ReplicaSet`은 삭제한다. 즉, replicas가 0인 `ReplicaSet`을 하나만 남긴다는 의미이다.  
   > 0이 된 `ReplicaSet`은 이전 버전으로 돌아가고 싶을 때 사용된다. 
   * Service yaml 파일
   ```yml
   apiVersion: v1
   kind: Service
   metadata:
     name: svc-1
   spec:
     # 연결할 Pod의 label과 동일하게 selector 지정
     selector:
       type: app
     ports:
       - port: 8080
         protocol: TCP
         targetPort: 8080
  ```
* ## Rolling Update
<img src="https://user-images.githubusercontent.com/50009240/141171621-f5a9139e-cc77-4f2a-a31e-3d43339041fc.png" width="500" height="210"> 

  * 위와 같이 서비스가 운영되고 있다고 하자
<img src="https://user-images.githubusercontent.com/50009240/141171700-9e143b38-6776-4294-96e3-1c737beee6bc.png" width="500" height="210">

  * Rolling Update로 서비스 업그레이드 과정
  1. `Deployment`의 `template`를 v2의 `Pod` 내용으로 변경한다.
  2. `replicas`값이 1인 `ReplicaSet`을 생성하고, 새로운 `Pod` 1개를 생성한다.
  3. 새로운 `Pod`의 label이 `Service`의 selector와 같으므로 연결된다.
  4. `Service`에서는  트래픽이 v1/v2 `Pod`에 분산되어 보내진다.
  5. 새로운 `Pod`가 생성되면 기존의(v1) `ReplicaSet`의 replicas값을 2에서 1로 변경한다. v1의 `Pod` 중 하나가 삭제된다. 
  6. 새로운(v2) `ReplicaSet`의 replicas를 2로 변경해 새로운 `Pod`를 생성한다. 
  7. 기존의(v1) `ReplicaSet`의 replicas를 0으로 변경해 남은 `Pod`를 모두 삭제한다.
  * ReCreate와 마찬가지로 기존의 `ReplicaSet`을 지우지 않고 배포를 종료하게 된다.
  * Rolling Update yaml 파일
  ```yml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: deployment-2
  spec:
    # selector, replicas, template 설정
    selector:
      matchLabels:
        type: app
    replicas: 2
    strategy:
      # 배포 방식
      type: RollingUpdate
    minReadySeconds: 10
    template:
      metadata:
        labels:
          type: app
      spec:
        containers:
        - name: container
          image: tmkube/app:v1
  ```
  > minReadySeconds는 기존 ReplicaSet의 replicas 수를 줄여 Pod를 삭제하고, 새로운 ReplicaSet에서 Pod를 생성하는 과정에서 대기할 시간을 설정한 값이다.  
  * 새로 생성된 ReplicaSet의 selector와 기존에 있던 Pod의 label이 같은데 그럼 둘이 연결되지 않나?
  > Deployment가 ReplicaSet을 만들 때 ReplicaSet의 selector와 Pod의 label만을 사용해 매칭하지 않고, 추가적인 label과 selector를 만들어주기 때문에 새로 생성된 ReplicaSet과 기존의 Pod는 연결되지 않는다.
# DaemonSet, Job, CronJob
<img src="https://user-images.githubusercontent.com/50009240/141173692-0342cbf6-48a0-4de2-9581-e3e8dde5a889.png" width="700" height="250">

* DaemonSet
  * 이전에 `Controller`는 NodeSchedule에 의해 `Node`에 `Pod`를 배치할 때 각 `Node`의 사용 가능한 자원량을 기준으로 비교적 여유로운 `Node`에 우선적으로 `Pod`를 배치했다.
  * 반면 `DaemonSet`은 `Node`의 자원 상태에 상관없이 모든 `Node`에 `Pod`를 하나씩 생성한다. 
  * 만약 `Node`가 10개면 각 `Node`에 하나씩 총 10개의 `Pod`를 생성한다.
  * 위처럼 각각의 `Node`마다 설치해 사용해야하는 서비스 종류
    * Prometheus: 성능 모니터링을 위한 서비스
    * Fluentd: 특정 `Node`에 발생하는 장애를 대비하기 위해 로깅을 담당하는 서비스
    * GlusterFS: `Node`들 내에 Storage로 활용하기 위한 서비스
 * Job
   * `Node1`에 직접 만든 `Pod`, `ReplicaSet`을 통해 만든 `Pod`, `Job`을 통해 만든 `Pod`가 있다고 할 때 `Node1`에 장애가 발생했다고 가정
   * 직접 만든 `Pod`에도 장애가 발생해 해당 서비스는 더이상 유지할 수 없다.
   * `Controller`에 의해 생성된 `Pod`는 장애가 발생하면 ReCreate해 다른 `Node`에 재생성 된다.
   * `ReplicaSet`을 통해 재생성된 `Pod`는 작동하지 않으면 Restart시켜 주기 때문에 해당 `Pod`에 있는 서비스는 어떤 상황에서도 유지된다.
     * ReCreate: `Pod`를 다시 생성하기 때문에 `Pod`의 이름, IP들이 변경된다.
     * ReStart: `Pod` 내 컨테이너만 재기동 시켜준다.
   * `Job`을 통해 재생성된 `Pod`는 작동하지 않으면 종료된다. 
     * `Pod`가 삭제되진 않고, 자원을 사용하지 않는 상태로 머무른다. 
     * `Pod` 안의 log를 통해 작업 결과를 볼 수 있다.
     * 이후 필요가 없으면 직접 `Pod`를 삭제하면 된다.
* CronJob
  * `Job`들을 주기적인 시간에 따라서 생성하는 역할을 한다.
  * `Job`을 여러개 묶어 `CronJob`으로 만들어 `CronJob`단위로 특정 시간에 반복적으로 실행할 목적으로 사용한다. 
  * 주기적인 Backup, Checking, Messaging 등에 사용
<img src="https://user-images.githubusercontent.com/50009240/141172956-6e48e792-fd13-4870-9976-7a1f4d9b62df.png" width="1000" height="250">

* ## DaemonSet
  * `DaemonSet`은 selector와 template이 있고, template 내용으로 모든 `Node` 각각에 `Pod`를 생성한다. 생성된 `Pod`들은 label을 통해 `DaemonSet`과 연결된다.
  * 특정 `Node`에만 `Pod`를 생성하고 싶지 않다면, `nodeSelector`를 지정해 해당 label을 가진 `Node`에만 `Pod`를 생성하도록 한다.
    * ex) DaemonSet에 있는 Pod는 `os:ubuntu`에서 실행할 수 없다고 가정
    * nodeSelector로 `os:centos`를 지정해 해당 label을 가진 `Node1`,`Node2`에서만 Pod가 생성되도록 한다.
  * `hostPort`를 사용해 `Node`로 들어온 트래픽을 바로 해당 Node 내 `Pod`에 전달할 수 있다.
    * 이전에 배운 `NodePort`라는 `Service`를 만들면 `Node`에 포트가 할당되어 트래픽이 해당 포트를 통해 `Node`로 들어오고, 이를 `Service`로 전달해 `Service`에서 `Pod`로 트래픽을 라우팅했다.
    * `hostPort`를 사용하면 `NodePort`없이 똑같은 결과를 얻을 수 있다.
  * DaemonSet yaml 파일
  ```yml
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: daemonset-1
  spec:
    # 연결할 Pod의 label
    selector:
      matchLabels:
        type: app
    # 생성할 Pod의 정보
    template:
      metadata:
        labels:
          type: app
  spec:
    # Pod를 생성할 Node의 label
    nodeSelector:
      os: centos
    containers:
    - name: container
      image: tmkube/app
      # hostPort로 들어온 트래픽은 해당 Pod 내 8080 Port의 컨테이너로 연결
      ports:
      - containerPost: 8080 
        hostPort: 18080
  ```
* ## Job
  * `Job`에도 template과 selector가 존재한다. template에는 특정 작업만 수행한 후 종료되는 `Pod`들을 담는다. selector는 자동으로 생성된다.
  * 일반적으로 template을 통해 하나의 `Pod`를 생성하고, 해당 `Pod`가 일을 다하면 `Job`도 종료되지만, `completions`를 사용하면 N개의 `Pod`들은 순차적으로 실행시켜서 모두 작업이 끝나야 `Job`이 종료된다. 
  * `parallelism`을 사용하면 지정한 숫자 단위로 `Pod`가 생성된다.
  * `activeDeadlineSeconds`는 해당 시간이 지나면 실행되고 있던 모든 `Pod`들은 삭제된다. 
    * 10초 걸리는 Job을 실행하는데 30초가 되도록 실행이 끝나지 않으면 Job의 `Pod`들을 모두 삭제하고 자원을 해제한다.
  * Job yaml 파일
  ```yml
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: job-1
  spec:
    # 6개의 Pod가 모두 끝나야 Job 종료
    completions: 6
    # 2개씩 Pod 생성
    parallelism: 2
    # Job의 수행이 30초를 초과하면 모든 Pod 삭제
    activeDeadlineSeconds: 30
    template:
      spec:
        # Never, Onfailure 옵션 존재
        restartPolicy: Never
        containers:
        - name: container
          image: tmkube/init
  ```
* ## CronJob
  * `jobTemplate`가 있어서 이 내용으로 `Job`을 만들고 schedule의 시간을 주기로 `Job`을 생성한다. schedule 주기로 생성된 `Job`은 `Pod`를 생성한다. 
  * ConcurencyPolicy
  <img src="https://user-images.githubusercontent.com/50009240/141173253-e53be235-0ff9-49c3-b55b-9b2f12fd57ff.png" width="500" height="190">
  
    * Allow: default값으로, 사전에 생성한 `Pod`의 상태(실행 중/종료)에 상관없이 schedule 주기가 되면 새로운 `Job`을 생성한다.
    * Forbid: 사전에 생성한 `Pod`가 종료되지 않고 실행 중이라면, 주기가 되어도 새로운 `Job`를 생성하지 않고 skip한다. 실행 중이던 `Pod`가 종료되는 즉시 다음 주기에 `Job`을 생성한다.
    * Replace: 사전에 생성한 `Pod`가 자신의 주기에 작업을 끝내지 못했다면, 다음 주기에 새로운 `Pod`를 만들어 이전 주기의 `Job`과 연결한다.
  * yaml 파일
  ```yml
  apiVersion: batch/v1
  kind: CronJob
  metadata:
    name: cron-job
  spec:
    # 1분마다 `Job` 생성
    schedule: "*/1 * * * *"
    # Allow 1분 주기로 Job 생성
    concurrencyPolicy: Allow
    jobTemplate:
      spec:
        template:
          spec:
            restartPolicy: Never
            containers:
            - name: container
              image: tmkube/app
  ```
