Basic object
===
* [Pod](#Pod)
* [Service](#Service)
* [Volume](#Volume)
* [ConfigMap, Secret](#ConfigMap,-Secret)
* [Namespace, ResourceQuota, LimitRange](#Namespace,-ResourceQuota,-LimitRange)

# Pod
<img src="https://user-images.githubusercontent.com/50009240/137975533-0d948b30-6223-47ea-b599-0d12ae6f61ce.png" height="300">

* ## Pod
  * `Pod` 안에는 하나의 독립적인 서비스를 구동할 수 있는 컨테이너 존재
  * 하나의 `Pod`에 여러 개의 컨테이너 담을 수 있다.
  * 컨테이너들은 서비스가 연결될 수 있도록 `port`를 가지고 있다.
    * 하나의 컨테이너가 `port`를 하나 이상 가질 수 있지만, 한 `pod` 내에서 컨테이너들끼리 `port`번호가 중복되면 안된다.
  * `Pod` 내 컨테이너들은 하나의 Host로 묶여있다고 볼 수 있다.
    * Pod 내 컨테이너끼리는 `localhost:{port}`로 접근할 수 있다.
  * `Pod`가 생성될 때 고유 `IP주소`가 할당 된다. 
    * 쿠버네티스 클러스터 내에서만 해당 IP로 접근 가능하고, 외부에서는 접근 불가
  * `Pod`에 문제가 발생하면 시스템이 감지해 `Pod`를 삭제하고, 재생성 한다. 이때 IP주소는 변경된다.
  * 휘발성 특징을 가진 IP주소  
  * yaml 파일
  ```yaml
  apiVersion: v1
  # 파일에서 정의하는 쿠버네티스 오브젝트 유형
  kind: Pod
  # Pod의 메타데이터 정보
  metadata:
  name: Pod-1
  # 쿠버네티스 오브젝트 유형에 대한 정보 정의
  spec:
    containers:
      # 컨테이너 이름
      - name: container1
        # 도커 허브에 등록된 이미지 태그값
        image: tmkube/p8000
        # 컨테이너가 노출시킬 포트
        port:
        - containerPost: 8000
      # 컨테이너 이름
      - name: container2
        # 도커 허브에 등록된 이미지 태그값
        image: tmkube/p8080
        # 컨테이너가 노출시킬 포트 
        ports:
        - containerPost: 8080
  ```
* ## Label
  * 목적에 따라 오브젝트들을 분리하고, 분류된 오브젝트들만 따로 골라서 연결하기 위해 사용한다.
  * `Key : Value`로 구성되어 있고, 한 Pod에는 여러 개의 Label 달 수 있다.
  * 예시) 위의 그림처럼 각 Pod에 label이 있다고 했을 때
    *  web화면을 보고 싶은 경우 `type:web` label이 달린 Pod들만 서비스에 연결
    * 상용환경 관련 정보만 얻고 싶은 경우 'loc:production` label이 달린 Pod들만 서비스에 연결
  * label에 따라 원하는 Pod 선택해 사용 가능
  * Pod의 yaml파일
  ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-2
      # key:value로 label 설정 가능
      labels:
        type: web
        lo: dev
      spec:
        containers:
        - name: container
          image: tmkube/init
    ```
  
  * Service의 yaml파일
  ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-1
    spec:
      # 연결할 Pod의 label을 selector로 설정
      selector:
        type: web
      ports:
        - port: 8080
  ```
* ## Node Schedule
  * Pod는 결국 여러 노드들 중에 한 노드에 올라가야 된다.
  * 방법1) 노드 직접 선택
    * 노드에 `label`을 달고 Pod들 만들 때 노드를 직접 지정할 수 있다.
    * Pod의 yaml파일
    ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: pod-3
      spec:
        # 연결하려는 노드 지정
        nodeSelector:'
          # 연결하려는 노드의 'key:value'형태 label 명시
          hostname: node1
        containers:
        - name: container
          image: tmkube/init
      ```
  * 방법2) k8s의 Node Scheduler
    * 각 노드의 사용가능한 자원량(memory...)과 Pod에서 필요한 자원량을 보고 Pod를 노드에 Scheduling
    * 노드에 연결할 Pod의 yaml파일에 필요한 자원량을 명시해줘야 한다.
      * 자원량을 보지 않고, Scheduling한다면 Pod 내 app에서 부화가 생길 때 노드 내 자원을 무한정 사용할 것이고, 노드 내 다른 Pod들은 자원이 없어서 다같이 죽기 때문
    * Pod의 yaml파일
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-4
    spec:
      containers:
      - name: container
        image: tmkube/init
        # Pod에서 사용될 자원량
        resources:
          # 필요한 메모리 크기
          requests:
            memory: 2Gi
          # 최대 허용 메모리 크기
          limits:
            memory: 3Gi
    ```
    * `memory`의 경우 limits 초과 시 바로 Pod를 종료시킨다.
    * `Cpu`의 경우 limits 초과 시 request로 낮춘다. (Pod는 종료되지 않음)
    * memory의 초과는 프로세스 간의 치명적 문제를 일으키므로 바로 Pod를 종료한다.
    * Cpu는 초과해도 프로세스간 작동에 문제는 없으므로 Pod를 종료시키지 않는다.
# Service
<img src="https://user-images.githubusercontent.com/50009240/137975918-d763db3b-f768-4ab0-88e7-6784ca6254cd.png">

* ## Service
  * `Service`는 자신의 `ClusterIP`를 갖고 있다.
  * Service를 Pod에 연결하면 Service의 IP를 통해서도 Pod에 접근할 수 있다.
  * Pod에도 똑같이 Cluster 내에서 접근할 수 있는 IP주소가 있는데 왜 굳이 서비스를 통해 Pod에 접근할까?
    > `Pod`는 시스템/성능 장애로 언제든지 죽을 수 있고, 재생성되도록 설계되어 있는 오브젝트이다.
    > Pod의 IP는 재생성 시 변경되므로 Pod의 IP는 신뢰성이 떨어진다.
    > `Service`는 사용자가 직접 지우지 않는 한 삭제되거나, 재생성되지 않으므로
    > 따라서 신뢰성 있는 Service의 IP로 접근하면 항상 연결되어 있는 Pod로 접근할 수 있다.
  * Service의 종류에 따라 Pod의 접근을 도와주는 방식이 다르다. 
  * Service yaml파일
  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: svc-1
  spec:
    # 서비스와 연결할 오브젝트(Pod) 설정
    selector:
      # 분산할 오브젝트의 이름 
      app: pod
    ports:
      # 서비스의 포트
      - port: 9000
        # 파드의 포트
        targetPost: 8080
    # 서비스 유형(default = ClusterIP)
    # ClusterIP 사용 시 생략 가능
    type: ClusterIP
  ```
  * Pod의 yaml파일
  ```yaml
  apiVersion: v1
  # 오브젝트 유형
  kind: Pod
  # 메타데이터 정보
  metadata:
    # 오브젝트 이름
    name: pod-1
    # 연결할 서비스의 label
    labels:
      app: pod
  spec:
    containers:
    # 컨테이너 이름
    - name: container
       # 도커 허브에 등록된 이미지 태그값
       image: tmkube/app
       # 컨테이너가 노출시킬 포트
       ports:
       - containerPost: 8080
  ```
* ## NodePort
  * NodePort로 만들어도 `Service`에는 기본적으로 `ClusterIP` 할당되므로 ClusterIP의 성격을 포함한다.
  * 쿠버네티스 클러스터에 연결되어 있는 모든 노드들한테 똑같은 Port가 할당된다.
    * Pod가 있는 노드뿐만 아니라, 모든 노드에 port가 할당된다.
  * 외부로부터 노드에 상관없이 해당 `IP의 Port`로 접속하면 Service에 연결되고, 자신과 연결되어 있는 Pod에 트래픽을 전달한다.
  *  대부분 Host IP는 주로 내부망에서 사용되고, 외부와는 데모나 임시 연결용으로 사용
  * Service의 yl 파일
  ```yaml
  apiVersion: v1
  # 오브젝트 유형
  kind: Service
  metadata:
    name: svc-2
  spec:
    selector:
      app: pod
    ports:
      # 서비스 포트
      - port: 9000
        # 파드 포트
        targetPort: 8080
        # 모든 노드에 할당한 포트
        # 30000~32767 사이 번호로 포트 할당, 생략하면 자동 할당됨
        nodePort: 30000
    # 서비스 유형
    type: NodePort
    # 트래픽 분산 제어 옵션
    # externalTrafficPolicy: Local
  ```
  * Service 입장에서는 어떤 노드에게 온 트래픽인지 상관없이 연결되어 있는 Pod들에게 트래픽을 전달한다.
    * 각 노드에 Pod가 하나씩 올라가 있는 경우 Node1의 IP로 접근을 하더라도 Service는 Node2의 Pod에 트래픽 전달이 가능하다.
  * `externalTrafficPolicy: Local`을 추가하면 특정 노드 port의 IP로 접근하는 트래픽은 Service가 해당 노드 위에 있는 Pod에게만 트래픽을 전달한다.
* ## Load Balancer
  * 기본적으로 `NodePort`의 성격을 포함한다.
  * `Load Balancer`가 추가돼 각각의 노드에 트래픽을 분산시켜주는 역할을 한다.
  * `Load Balancer`에 접근하기 위한 외부 접속 IP는 별도로 할당해줘야 한다.
    * 외부 접속 IP를 할당해주는 플러그인 필요
    * GCP, AWS, Azure, OpenStack ... 플랫폼을 사용해 외부 접속 IP 할당
  * 외부에 서비스를 노출하는데 사용
  * Service의 yaml파일
   ```yaml
   apiVersion: v1
   # 오브젝트 유형
   kind: Service
   metadata:
     name: svc-3
   spec:
     selector:
       app: pod
     ports:
       - port: 9000
         targetPort: 8080
     # 서비스 유형
     type: LoadBalancer
   ```
# Volume
<img src="https://user-images.githubusercontent.com/50009240/137976512-f7afa9e4-6503-496b-b225-57af5cfc597e.png" height="250">

* ## emptyDir
  * 컨테이너끼리 데이터를 공유하기 위해 사용한다.
  * 최초로 볼륨이 생성될 때는 비어있는 상태이므로 `emptyDir`
  * `Volume`은 Pod 내 존재하므로 Pod 생성시 만들어지고 삭제시 없어진다.
    * Volume에는일시적인 사용목적에 의한 데이터만 넣는 것을 권장 
  * 예시)  Container1이 웹역할의 서버이고, Container2이 백엔드 서버인 경우
    1. 웹서버로 받은 파일을 마운트가 된 `Volume`에 저장
    2. 백엔드 컨테이너도 해당 'Volume`을 마운트
    3. 두 서버가 `Volume`을 자신의 local에 있는 파일처럼 사용하므로 두 서버가 서로 파일을 주고받을 필요 없이 사용 가능
  * Pod의 yaml 파일
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-volume-1
  spec:
    # 2개의 컨테이너 존재
    containers:
      - name: container1
        image: tmkube/init
        # 볼륨 마운트
        volumeMounts:
        - name: empty-dir
          # 컨테이너가 해당 경로로 볼륨 연결
          mountPath: /mount1
      - name: container2
        image: tmkube/init
        # 볼륨 마운트
        volumeMounts:
        - name: empty-dir
          # 컨테이너가 해당 경로로 볼륨 연결
          mountPath: /mount2
      # 각 컨테이너들의 mountPath는 달라도 됨
      # 볼륨 설정
      volumes:
      - name: empty-dir
        # 볼륨 속성
        emptyDir: {}
  ```
* ## hostPath
  * `Pod`들이 올라가져 있는 `Node`의 Path를 볼륨으로써 사용한다.
  * Host Path를 각각의 Pod들이 마운트해서 공유하기 때문에 Pod가 죽어도 해당 노드에 있는 데이터는 사라지지 않는다.
  * Pod가 죽어서 재생성될 때 다른 노드에 생성되는 경우 이전 노드의 Volume을 마운트 할 수 없다.
    * hostPath이기 때문에 자신의 파드가 올라가져 있는 노드의 볼륨만 마운트 할 수 있다.
    * Node추가시마다 똑같은 이름의 경로를 만들어서 직접 노드에 있는 path끼리 마운트시켜주면 되긴 됨.
    * 위는 쿠버네티스가 해주는 역할은 아니고, 운영자가 직접 마운트 시켜줘야됨. 추천은 놉
  * `hostPath`는 `Pod`의 데이터를 저장하기 위한 용도가 아니라, `Node`에 있는 데이터를 `Pod`에서 쓰기 위한 용도이다.
  * Pod의 yaml파일
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-volume-2
  spec:
    containers:
      - name: container
        image: tmkube/init
        # 볼륨 마운트
        volumeMounts:
        - name: host-path
          # 컨테이너의 볼륨 연결 경로
          mountPath: /mount1
    # 볼륨 설정
    volumes:
    - name: host-path
      # hostPath 속성
      hostPath:
        # Pod를 생성하기 전에 해당 Node에 경로가 있어야함
        path: /node-v
        type: Directory
  ```
* ## PVC/PV
  * `Pod`에 영속성있는 `Volume`을 제공하기 위한 기능
  * `PV(퍼시스턴트 볼륨)`
    * 파드가 사용할 스토리지의 크기 및 종류를 정의한다.
    * 클러스터 운영자 영역에 해당한다.
  * `PVC(퍼시스턴트 볼륨 클레임)`
    * PV를 동적으로 확보하기 위해 사용한다.
    * 클러스터 사용자 영역에 해당한다.
  * 구조: Pod -> PVC -> PV -> Volume
    * User(Pod, PVC): Pod의 서비스를 만들고 배포
    * Admin(PV, Volume): 쿠버네티스 운영
  * PV의 yaml파일
  ```yaml
  apiVersion: v1
  kind: PersistentVloume
  metadata:
    name: pv-01
  spec:
    nfs:
      server: 192.168.0.xxx
      path: /sda/data
    iscsi:
      targetPortal: 163.180.11
      iqn: iqn.200.qnap:....
      lun: 0
      fsType: ext4
      readOnly: no
      chapAuthSession: true
    gitRepo:
      repository: github.com/..
      revision: master
      directory: .
  ```
  > 각각의 Volum에 따라 속성이 다르다.  
  > `PV`를 전문적으로 관리하는 Admin이 `PV`를 만들어 놓으면 User는 이를 사용하기 위해 `PVC` 만든다.
  * PVC의 yaml파일
  ```yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: pvc-01
  spec:
    accessModes:
      # 읽기 쓰기 모드
      - ReadWriteOnce
    resources:
      requests:
        # 용량이 1Gi인 볼륨 할당 요청
        storage: 1Gi
    # 현재 만들어져 있는 `PV`들 중에서 선택
    storageClassName: ""
  ```
  * Pod의 yaml파일
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-volume-3
  spec:
    containers:
    - name: container
      image: image/init
      volumeMounts:
        # volumes: 에서 만든 vloume을 컨테이너에서 사용
        - name: pvc-pv
          mountPath: /volume
  volumes:
    - name: pvc-pv
      persistentVolumeClaim:
        # 앞서 만든 PVC 이름 연결
        claimName: pvc-01
  ```
  * PVC/PV 과정 정리
    1. 최초 Admin이 PV 정의 생성
    2. 사용자가 PVC 생성
    3. 쿠버네티스가 PVC 내용에 맞는 적절한 PV로 연결
      * `PVC`의 .yaml파일의 spec 아래 `capacity`, `accessModes` 보고 `PV`의 .yaml파일과 비교해 연결
      ```yaml
      # ... 생략
      spec:
        capacity:
          storage: 1G
        accessModes:
          - ReadWriteOnce
      # ...
      ```
    4. Pod 생성시 PVC 마운팅
# ConfigMap, Secret
<img src="https://user-images.githubusercontent.com/50009240/137977093-3a1885d0-5d0d-4605-ad79-8eaeab0ff0c1.png" width="700" height="370">

* 사용 목적
  * 개발환경과 상용환경이 있을 때 A Service에서는 일반 접근과 보안 접근을 지원하고 있다.  
  * 개발환경에서는 이 보안접근을 해제(SSH: False)할 수 있는 옵션이 존재하고, 보안접근시 접근`User`와 `Key`를 설정할 수 있다.  
  * 서비스를 상용환경으로 배포하는 경우 다시 보안접속으로 설정(SSH: True)을 해야되고, `User`와 `Key`값도 변해야 한다.  
  * 하지만, `SSH`, `Uesr`, `Key`값은 컨테이너 이미지에 들어있기 때문에 이를 변경하기 위해선 개발환경과 상용환경의 컨테이너 이미지를 각각 관리해야 한다.
    * 값 몇 개 때문에 큰 용량의 이미지를 별도로 관리하는 것은 꽤나 부담되는 일이다.  
  * 따라서 컨테이너 이미지를 별도로 관리하지 않고,`ConfigMap`과 `Secret` 오브젝트를 사용해 환경에 따라 변하는 값들은 외부에서 결정할 수 있도록 한다.  
* 사용 과정
  * 분리해야되는 일반적인 상수들을 모아서 `ConfigMap`을 생성한다.
  * password, 인증키와 같이 보안적인 관리가 필요한 값을 모아서 `Secret`을 생성한다.
  * `Pod` 생성 시 컨테이너에 `ConfigMap`과 `Secret` 오브젝트 연결해 `Container`의 환경변수로 사용할 수 있다.
  * `ConfigMap`, `Secret`와 연결해 사용할 컨테이너 이미지를 만들어 놓으면 개발환경, 상용환경에 모두 사용 가능하다.
* ConfigMap/Secret 특성
  * `Secret`에 `value`를 넣을 때 Base64로 인코딩해서 넣어야 한다. `Pod`로 주입이 될 때는 자동으로 디코딩 되어 환경변수로 사용된다. 
  * 일반적인 오브젝트 값들은 `K8S DB`에 저장되지만, `Secret`은 `메모리`에 저장된다. 
    * 메모리에 저장하는 것이 보안적인 면에서 좋지만, `Secret`을 너무 많이 생성하면 시스템 자원에 영향을 미친다.
  * `Config`에는 key-value를 무한으로 넣을 수 있지만, `Secret`에는 최대 1MB의 내용을 넣을 수 있다.
<img src="https://user-images.githubusercontent.com/50009240/137977609-34e95450-20cf-4504-b692-656afc614e9c.png" height="300">

* ## Env(Literal)
  * `ConfigMap`과 `Secret`의 데이터로 상수를 정의
  * ConfigMap의 yaml 파일
  ```yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    # ConfigMap 이름
    name: cm-dev
  # key-value 형태의 상수 데이터
  data:
    SSH: False
    User: dev
  ```
  * ConfigMap의 yaml 파일
  ```yaml
  apiVersion: v1
  kind: Secret
  metadata:
    # Secret 이름
    name: sec-dev
  # key-value 형태의 상수 데이터
  # value는 Base64로 변환해 넣음
  data:
    key: MTIzNA==
  ```
  * Pod의 yaml 파일
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-1
  spec:
    containers:
      - name: container
        image: tmkube/init
        # envFrom 속성
        envFrom:
        # configMap 참조
        - configMapRef:
          # 가져올 ConfigMap의 이름
          name: cm-dev
        # secret 참조 
        - secretRef:
          # 가져올 Secret의 이름
          name: sec-dev
  ```
* ## Env(File)
  * `ConfigMap`과 `Secret`의 데이터로 파일을 정의.
  * file자체를 `ConfigMap`으로 담을 수 있다.
  * *파일 이름(file.txt)은 `key`, 파일 내용은 `value`가 된다.*
  * `ConfigMap`, `Secret`를 파일로 생성하는 것은 대시보드에서 지원하지 않기 때문에 직접 master에 접속하여 생성해야 한다.
  * `ConfigMap`을 파일로 생성
    * 이름이 cm-file인 ConfigMap을 생성하고, file.txt를 넣는다는 의미
    ```
    kubectl create configmap cm-file --from-file=./file.txt
    ```
  * `Secret`을 파일로 생성
    * 아래 명령으로 파일의 내용이 Base64로 변경된다. 이미 Base64로 변경된 상태라면 두 번 인코딩하는 것이므로 주의
    ```
    kubectl create secret generic sec-file --from-file=./file.txt
    ```
  * pod의 .yml파일
    ```yml
    apiVersion: v1
    kind: Pod
    metadata:
      name: file
    spec:
      containers:
        - name: container
          image: tmkube/init
          # 컨테이너의 환경변수 설정
          env:
            # 환경변수 이름
            - name: file
              # 파일의 내용을 가져옴
              valueFrom:
                # ConfigMap의 Key를 참조
                configMapKeyRef:
                  # 참조할 ConfigMap 이름
                  name: cm-file
                  # file.txt Key에 대한 value를 환경변수에 넣는다.
                  key: file.txt
    ```
* ## Volume Mount(File)
  * `ConfigMap`, `Secret`에 존재하는 file 데이터를 환경 변수로 사용하지 않고, `Volume`을 Mount해 사용
  * `ConfigMap`, `Secret`을 생성하는 과정은 Env(File)과 동일하나, `Pod`를 생성할 때 컨테이너 안에 mount할 경로를 지정해 해당 경로 안에 파일을 mount 할 수 있다.
  * Pod의 .yml
    ```yml
    apiVersion: v1
    kind: Pod
    metadata:
      name: mount
    spec:
      containers:
        - name: container
          image: tmkube/init
          # 컨테이너 안에 volume을 mount
          volumeMounts:
          - name: file-volume
            # mount할 경로
            mountPath: /mount
      # mount할 volume 정의
      volumes:
      - name: file-volume
        # volume 안에 configMap을 담음
        configMap:
          name: cm-file
    ``` 
  * File mount vs File 환경변수
  * `Pod`를 생성한 뒤 `ConfigMap`의 내용을 변경하는 경우
  * `환경변수` 방식은 한 번 주입을 하면 끝이므로 configMap의 내용이 변경되더라도, `Pod`내 `Container`의 환경변수에는 아무런 영향이 없다.
    * `Pod`가 죽어서 재생성이 되야지만 변경된 ConfigMap의 데이터를 다시 받아와 환경변수를 수정할 수 있다.
  * `File mount`는 *mount가 원본과 연결시켜놓는다는 개념*이기 때문에 ConfigMap의 내용이 변경되면 Pod에 mount된 내용도 변한다. 
# Namespace, ResourceQuota, LimitRange
<img src="https://user-images.githubusercontent.com/50009240/137977838-3d23efd2-f84b-4fe5-8258-81510bc20ea5.png" width="600" height="300">

* 사용 목적
  * `kubernetes Cluster`에는 전체 사용할 수 있는 자원이 있다.
    * 일반적으로 Memory, CPU 자원이 있다.
  * 클러스터 안에는 여러 `Namespace`들을 생성할 수 있다.
  * Namespace 안에는 여러 `Pod`들을 생성할 수 있다.
  * 각 Pod는 클러스터 자원을 공유해 사용한다. 
  * 만약 한 Namespace 내 Pod가 클러스터에 남은 자원을 모두 사용해버리면 다른 Pod들은 더 이상 사용가능한 자원이 없어서 문제가 발생한다.
  * 이러한 문제를 해결하기 위해 `Resource Quota`가 존재한다. 
  * 각 Namespace마다 `Resource Quota`를 통해 사용 가능한 클러스터 자원의 최대 한계를 설정할 수 있다.
  * Pod 자원이 최대 한계를 넘을 수 없으므로 자신의 namespace의 자원을 다 쓰더라도 다른 Namespace에 있는 Pod들에 영향을 주지 않는다.
  * 한 Pod의 자원 사용량이 너무 크면 다른 Pod들이 해당 Namespace에 더 이상 들어올 수 없게 된다.
  * 한 Pod가 Namespace내 자원을 모두 사용하는 것을 막기 위해 `Limit Range`를 사용한다.
  * `Limit Range`를 통해 Namespace에 들어오는 Pod의 크기를 제한할 수 있다.
  * 필요한 자원량이 Limit Range보다 작은 Pod만 해당 Namespace에 들어올 수 있다. Limit Range보다 크면 Namespace에 들어올 수 없다.
  * `Limit Range`와 `Resource Quota` 오브젝트는 `Namespace` 뿐만 아니라, `kubernetes Cluster`에도 적용해 전체 자원에 대한 제한을 걸 수 있다.
<img src="https://user-images.githubusercontent.com/50009240/137978126-40a6ef70-372b-48a8-82f4-896973e0f630.png" height="300">

* ## Namespace
  * 한 `Namespace` 내에서는 같은 종류의 오브젝트의 이름이 중복될 수 없다.
    * Namespace 내에서는 `Pod`의 이름이 중복될 수 없다. 
  * 다른 Namespace 내에 있는 자원과 분리되어 관리한다. 
    * Namespace1에 `Pod`가 있고, Namespace2에 `Service`가 있다고 할 때, 두 오브젝트는 연결이 불가능하다. 
    * `Pod`와 `Service`는 `Namespace`가 다르기 때문에 `label`과 `selector`가 같아도 연결되지 않는다.
    * Namespace가 같다면 `Pod`에는 lavel을 달고, `Service`에는 selector를 달아 연결 가능
  * 대부분의 자원들은 해당 자원을 만든 `Namespace`에서만 사용할 수 있다.
    * `PV`나 `Node`와 같이 모든 Namespace에서 공용으로 사용되는 오브젝트도 있다.
  * `Namespace`를 삭제하면 해당 `Namespace` 내 자원들도 모두 삭제된다.  
  * yaml 파일
  ```yml
  # Namespace의 yml file
  apiVersion: v1
  kind: Namespace
  metadata:
    name: nm-1
    
  # Pod의 yml file
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-1
    # 자신이 속한 namespace 지정
    namespace: nm-1
    lables:
      nm: pod1
  spec:
    containers:
  ```
  ```yml
  # Namespace의 yml file
  apiVersion: v1
  kind: Namespace
  metadata:
    name: nm-2
  
  #Service의 yml file
  apiversion: v1
  kind: Service
  metadata:
    name: svc-1
    # 자신이 속한 namespace 지정
    namespace: nm-2
  spec:
    selector:
      nm: pod1
    ports:
  ```
* ## ResourceQuota
  * `Namespace`의 자원 한계를 설정하는 오브젝트
  * 제한하고자 하는 자원을 명시해 `Namespace`와 연결
  * `ResourceQuota`가 지정되어 있는 `Namespace`에 `Pod`를 생성할 때 `Pod`는 무조건 사용할 자원량을 명시해야 한다. 
    * `Pod`에 사용할 자원량이 명시되어 있지 않으면 해당 `Namespace`에 생성할 수 없다.
  * 현재 `Namespace`에서 사용 가능한 자원량보다 큰 자원을 요구하는 `Pod`는 해당 `Namespace`에 생성되지 않는다.
  * 제한 가능 자원: cpu, memory, storage
  * 개수 제한 가능한 오브젝트: Pod, Service, ConfigMap, PVC ...  
  * yaml 파일
  ```yml
  # ResourceQuota의 yml file
  apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: rq-1
    # ResourceQuota를 할당할 Namespace 설정
    namespace: nm-1
  spec:
    hard:
      # 제한할 자원의 종류와 한계치 설정
      requests.memory: 3Gi
      limits.memory: 6Gi
      
  # Pod의 yml file
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-2
  spec:
    containers:
    - name: container
      image
  tmkube/app
    # Pod에도 사용할 자원량 설정
    resources:
      requests:
        memory: 2Gi
      limits:
        memory: 2Gi
  ```
* ## LimitRange
  * `LimitRange`는 각각의 `Pod`마다 `Namespace`에 들어올 수 있는지 자원 체크
    * min: `Pod`에서 설정되는 자원의 최소 한계치
    * max: `Pod`에서 설정되는 자원의 최대 한계치
    * maxLimitRequestRatio: request값과 limit값의 최대 비율 설정
      * 예시) `Pod2`는 request, limits의 비율이 4이므로 해당 `Namespace`에 들어올 수 없다.
    * defaultRequest, default: `Pod`에 아무런 자원량이 명시되어 있지 않을 때 자동으로 defaultRequest, default만큼 자원 할당
  * yaml 파일
  ```yml
  apiVersion: v1
  kind: LimitRange
  metadata:
    name: lr-1
    # 할당할 namespace 명시
    namespace: nm-1
  spec:
    # 제한 설정
    limits:
    # type마다 지원되는 옵션들이 다르다.
    # Pod, PVC등의 타입이 있다.
    - type: Container
      min:
        memory: 1Gi
      max:
        memory: 4Gi
      defaultRequest:
        memory: 1Gi
      default:
        memory: 2Gi
      maxLimitRequestRatil:
        memory: 3
  ```


























