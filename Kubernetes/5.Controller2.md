# Controller
# StatefulSet
* 어플리케이션 종류에는 Stateless Application과 Stateful Application이 있다.
* ## Stateless Application
  * 대표적으로 Web Server
  * APACHE, NGINX, IIS가 있다.
  * App이 여러 개 배포되더라도 똑같은 서비스 역할을 한다.
  * App이 죽으면 같은 Service의 역할을 하는 App을 복제하고, 앱 이름은 중요하지 않다.
  * Volume이 반드시 필요하진 않다. 
  * Volume이 필요하면 Volume 하나에 App들을 연결하면 된다.
    * App의 log를 영구적으로 저장하고 싶은 경우 Volume이 필요한데 이때, Volume 하나에 모든 App이 연결해 log를 저장할 수 있다. 
  * 사용자가 접속한 네트워크 트래픽은 여러 App에 분산된다.
  * 단순 분산의 목적만 가진 연결이므로 **ReplicaSet** 사용
* ## Stateful Application
  * 대표적으로 Database
  * mongoDB, MaraDB, redis가 있다.
  * 각각의 App마다 자신의 역할이 있다.
  * mongoDB를 예시로 Primary는 메인 DB, Primary가 죽으면 Arbiter가 감지하여 Secondary가 Primary 역할을 할 수 있도록 변경해준다.
  * App이 죽으면 반드시 그 역할을 하는 App이 만들어지고, 이름도 똑같이 만들어져야 한다.
  * 각 App의 역할이 다르므로 Volume을 각각 써야 한다.
  * 사용자가 접속한 네트워크 트래픽은 각 App의 특징에 맞게 들어와야 한다.
    * Read/Write 트래픽은 Primary App으로, Read 트래픽은 트래픽 분산을 위해 Secondary App으로, Arbiter App은 감시역할이니 Primary App, Secondary App에 연결되어야 한다.
  * 역할에 따라 의도가 있는 연결이므로 **StatefulSet** 사용
* ## StatefulSet Controller
  * replicas에 따른 Pod 생성 시
    * ReplicaSet: 이름 뒷부분을 임의의 값으로 설정하고, Pod들이 동시에 생성된다.
    * StatefulSet: 이름 뒷부분을 0부터 순차적인 숫자로 설정하고, Pod들이 순차적으로 생성된다.
  * Pod 재생성 시
    * ReplicaSet: 새로운 이름으로 Pod 재생성
    * StatefulSet: 기존에 삭제된 Pod의 이름으로 Pod 재생성
  * replicas 0인 경우
    * ReplicaSet: 모든 Pod가 동시에 삭제된다.
    * StatefulSet: 인덱스가 높은 Pod부터 순차적으로 삭제된다.
* ## PersistentVolumeClaim, Headless Service
  * statefulSet에 PVC, Headless Service 연결하는 방법
  * Pod에 Volume 연결
    * ReplicaSet: Pod에 Volume을 연결하려면 `PVC`를 별도로 직접 생성하고, `ReplicaSet` template의 persistentVolumeClaim으로 연결할 PVC를 지정해 Pod와 PVC 연결
    * StatefulSet: template을 통해 Pod를 생성하고 volumeClaimTemplates를 통해 PVC가 동적으로 생성되어 Pod와 연결된다.  
  * replicas 3인 경우
    * ReplicaSet: 모든 Pod들이 똑같은 PVC에 연결된다.
      * 모든 Pod들은 persistentVolumeClaim 값으로 같은 PVC를 갖고 있기 때문
    * StatefulSet: volumeClaimTemplates에 의해 Pod가 추가될 때마다 새로운 PVC가 생성되어 연결된다.
      * Pod마다 각자의 역할에 따른 데이터를 저장할 수 있다.
      * Pod가 죽고 재생성되면, 기존에 연결되어 있던 PVC에 Pod가 연결된다.
  * PVC와 Pod의 위치
    * ReplicaSet: PVC가 Node1에 만들어지면, PVC에 연결할 Pod도 Node1에 있어야 한다.
      * template 안에 `nodeSelector: "node1"`을 지정해줘야 한다. 
    * StatefulSet: PVC가 동적으로 Pod와 같은 Node에 만들어지므로 알아서 모든 Node에 균등하게 배포된다.
  * replicas 0인 경우
    * ReplicaSet: Pod는 동시에 삭제되고, PVC는 삭제되지 않고 삭제하려면 사용자가 직접 삭제해야 된다.
    * StatefulSet: Pod는 순차적으로 삭제되지만, PVC는 삭제되지 않고 삭제하려면 사용자가 직접 삭제해야 된다.
  * Headless Service
    * `StatefulSet`을 만들 때 `serviceName: "Headless"` 속성을 넣을 수 있다.
    * 이 이름과 매칭되는 `Headless` 이름의 Service를 만들게 되면, Pod의 예측가능한 도메인 이름이 만들어진다. 
    * Internal 서버의 특정 Pod 입장에서 원하는 StatefulSet의 Pod와 연결할 수 있다.
      * 그림에서 `Pod-2.Headless`로 Pod 연결
    * 상황에 따라 StatefulSet의 Pod를 선택해서 접속할 수 있다.
  * PVC yaml 파일
  ```yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: replica-pvc1
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 1G
    storageClassName: "fast"
  ```
  > storageClassName을 사용해서 동적으로 PV를 만든다.
  * ReplicaSet yaml 파일
  ```yaml
  apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    name: replica-pvc
  spec:
    replicas: 1
    selector:
      matchLabels:
        type: web2
    template:
      metadata:
        labels:
          type: web2
      spec:
        nodeSelector:
          kubernetes.io/hostname: k8s-node1
        containers:
        - name: container
          image: kubetm/init
          volumeMounts:
          - name: storageos
            mountPath: /applog
        volumes:
        - name: storageos
          persistentVolumeClaim:
            claimName: replica-pvc1
        terminationGracePeriodSeconds: 10
  ```
  > 컨테이너에 volume mount 한다.  
  > /applog에 서비스 로그파일을 생성한다.  
  > `persistentVolumeClaim`의 claimName에 연결할 PVC 이름인 replica-pvc1이 지정되어 있다.  
  > nodeSelector로 PV나 Pod가 node1에 생성된다.  
  > replicas를 5로 했을 때, nodeSelector를 지정하지 않으면 Pod가 서로 다른 노드에 지정되어 오류가 발생한다. PVC와 Pod는 같은 노드에 존재해야 연결 가능
  * StatefulSet yaml파일
  ```yaml
  apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    name: stateful-pvc
  spec:
    replicas: 1
    selector:
      matchLabels:
        type: db2
    # headless Service를 위한 속성
    serviceName: "stateful-headless"
    template: 
      metadata:
        labels:
          type: db2
      spec:
        containers:
        - name: container
          image: kubetm/app
          volumeMounts:
          - name: volume
            mountPath: /applog
        terminationGracePeriodSeconds: 10
    volumeClaimTemplates:
    - metadata:
        name: volume
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1G
        storageClassName: "fast"
  ```
  > `volumeClaimTemplates`의 내용으로 PVC 동적 생성  
  > volumeClaimTemplates.metadata는 template의 volumeMounts의 name과 같아야 한다.
  > headless Service를 생성할 때 Service의 이름을 serviceName인 "stateful-headless"와 동일하게 설정해야 한다.
  > headless Service는 StatefulSet 정의에 따라 name, selector를 설정하고, clusterIP를 None으로 설정해 headless Service를 생성한다.
# Ingress
* 사용목적
* *Service LoadBalancing*
  * 쇼핑몰을 운영한다고 가정했을 때, 쇼핑 페이지, 고객센터, 주문 서비스를 각각 `Pod`로 만든다.
    * 이렇게 애플리케이션으로 나누면 쇼핑 페이지가 문제가 생겨도 다른 앱에는 영향이 없다.
  * 외부에서 연결할 수 있도록 각 `Pod`에 `Service`를 연결
  * 사용자에게는 각 `Service`마다 path가 다른 URL을 준다.
  * 일반적으로 각 path마다 서비스 IP를 이어주는 L4나 L7 스위치 장비가 있어야 한다.
  * 쿠버네티스는 Ingress가 위 역할을 한다.
  * Ingress는 사용자가 /로 들어오면 쇼핑 페이지로, /customer는 주문센서로, /order는 주문 서비스로 연결해준다.
  * Ingress를 사용하면 별도의 IP 로드밸런싱을 해주는 장비가 필요 없다.
* *Canary Upgrade*
  * V1의 앱들이 구동돼서 Service가 되고 있는 상태에서 테스트할 V2의 앱을 구동시킨다.
  * Ingress를 만들어서 V1, V2의 서비스와 연결
  * 사용자가 Ingress를 통해 접근했을 때 90%는 V1으로, 10%는 V2로 트래픽을 분산할 수 있다.
  * %의 수치를 변경하거나, 연결될 때 들어있는 헤더값에 따라 트래픽을 조정할 수 있다.
* ## Ingress Controller
  * Ingress 오브젝트는 쿠버네티스가 설치되어 있으면 바로 만들 수 있다.
  * Ingress 내용으로는 `Host`로 도메인 이름을 넣을 수 있고, 이 도메인으로 들어오는 트래픽을 `Path`에 따라 원하는 `Service`로 연결하도록 설정한다.
  * 쿠버네티스에서 이를 실행할 구현체를 만들기 위해선 별도 플러그인인 Ingress Controller를 설치해야 한다.
    * NGINX, Kong이 있다.<br>
  * NGINX를 설치했다고 하면, NGINX에 대한 `Namespace`가 생긴다.
  * Namespace에 `Deployment`와 `ReplicaSet`이 생성되면서 실제 Ingress의 구현체인 `NGINX Pod`가 생성된다.
  * Pod는 Ingress 룰이 있는지 확인하고 룰에 따라 Service에 연결해준다.
  * 룰에 따라 해당 Service에 트래픽이 전달되려면 외부에서 접속하는 트래픽은 NGINX Pod를 지나야 된다.0
  * 따라서 외부에서 접근할 수 있는 Service를 만들어서 NGINX Pod에 연결해줘야 한다.
    * 직접 쿠버네티스를 설치했다면, `NodePort`를 만들어 외부와 연결할 수 있다.
    * 클라우드 서비스를 이용하고 있다면, `LoadBalancer`를 만들어 외부와 연결할 수 있다. 
  * Ingress로 서비스에 트래픽 분산 과정
    1. Ingress 룰에서 지정한 도메인으로 접근이 들어온다. (www.app1.com)
    2. Service를 통해 NGINX Pod로 트래픽이 들어온다.
    3. Ingress 룰(path)에 따라 Service와 Pod로 접근
  * Ingress를 추가해 다른 도메인을 주고 path없이 바로 Service로 연결할 수도 있다.
    * 현재 NGINX가 설치되어 있다면 바로 Ingress가 인식되어 지정된 Service에 연결된다.
  * 도메인을 달리하여 여러 개의 Ingress를 생성할 수 있다.
# Ingress 기능
* ## Service Loadbalancing
  * 각각의 업무별로 `Pod`와 `Service`를 만든다.
  * 사전에 NGINX Controller를 설치하고 NGINX Pod가 외부와 연결되도록 `NodePort` Service를 만들어 Pod와 연결해준다.
  * master의 hostIP인 *192.168.0.30:30431* 로 접속하면 Pod의 80번 포트로 트래픽이 전송된다.
  * 위 환경을 구성한 상태에서 Ingress를 만들고 룰을 각 path마다 의도하는 서비스로 매칭한다.
  * 별도의 도메인 이름을 주지 않았기 때문에 사용자가 IP주소로 접속하면 path에 따라 서비스로 연결된다.
  * Ingress yaml 파일
  ```yaml
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: service-loadbalancing
  spec:
    rules:
    - http:
        paths:
          - path: /
            backend:
              serviceName: svc-shopping
              servicePort: 8080
          - path: /customer
            backend:
              serviceName: svc-customer
              servicePort: 8080
          - path: /order
            backend:
              serviceName: svc-order
              servicePort: 8080
  ```
  > rules를 보면 path: /, path: /customer, path: /order 별로 다른 서비스에 연결한다.
* ## Canary Upgrade
  * www.app.com 도메인 이름으로 사용자가 접근하면 `svc-v1`이름의 Service로 연결되도록 구성되어 있다.
  * 사용자에게 App이 운용되고 있는 상황에서 테스트할 Pod와 Service를 띄운다.
  * Ingress를 하나 더 만들어서 Host(도메인 이름)은 www.app.com으로 똑같이 설정하고, serviceName을 `svc-v2`로 설정한다.
  * 새로 만든 Ingress에 `@weight:10%` 어노테이션을 주면 위 도메인 이름으로 접근하는 트래픽의 10%sms V2로 흘러들어 간다.
  * 특정 나라별로 테스트하고 싶은 경우 `@header` 옵션을 이용하면 해당 나라의 트래픽은 100% V2 Pod로 연결된다.
    * ex) @header 옵션을 사용해 `Language: kr`이면 100% V2 Pod에 연결하도록 설정
* ## Https
  * Ingress를 통해서 https로 연결할 수 있도록 인증서 관리가 가능하다.
  * Pod 자체에서 인증서 관리를 하기 힘들 때 사용한다.
  * NGINX Pod로 https를 사용하려면 *443* 포트로 연결해야 한다.
  * Ingress를 만들 때 Host 도메인 이름과 Service 이름을 입력하고, `tls` 옵션을 준다.
  * `tls`에 secretName으로 실제 `Secret` 오브젝트를 연결한다. 
  * Secret 오브젝트 안에는 데이터 값으로 인증서를 담고 있다.
  * 이렇게 구성하면 사용자가 도메인 이름 앞에 *https*를 붙여야만 접근할 수 있다.  
  * Ingress - SSL yaml 파일
  ```yaml
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: https
  spec:
    tls:
    - hosts:
      - www.https.com
      secretName: secret-https
    rules:
      - host: www.https.com
        http:
          paths:
          - backend:
              serviceName: svc-https
              servicePort: 8080
  ```
  > tls 옵션에 host 이름과 screateName을 넣는다.  
  > rules에는 host와 service를 지정한다.
# Autoscaler - HPA
* ## Autoscaler 종류
  * 쿠버네티스의 Autoscaler에는 세 가지 종류가 있다.
  * `HPA`: Pod의 개수를 늘리는 방법
  * `VPA`: Pod의 리소스를 증가시키는 방법
  * `CA`: 클러스터에 노드를 추가하는 방법
  ### HPA
  * Controller가 있고 replicas 수치에 따라 Pod가 만들어져 운영되고 있는 상태
  * Service가 연결되어 모든 트래픽이 해당 Pod로 흐르는 상황
  * 트래픽이 많아져서 Pod 내에 있는 리소스를 모두 사용하게 된 경우 조금 더 트래픽이 증가하면 Pod는 죽을 수 있다. 
  * 사전에 `HPA`를 연결하고 Controller에 연결했다면, `HPA`가 위험한 상황에 컨트롤러의 *replicas를 2로 늘려준다.*
  * 이를 통해 Pod가 늘어나 자원이 수평적으로 늘어나는데 이를 `Scale Out`이라고 한다.
  * 트래픽이 감소하여 리소스 사용량이 줄면 Pod가 삭제되는데, 이를 `Scale In`이라고 한다.
  * Pod가 증가했기 때문에 트래픽은 각각 50%로 분산되어 자원이 배분되고, 안정적인 Service를 유지할 수 있게 된다.<br>
  * 고려해야할 사항
    * HPA는 장애처리를 위한 기능이므로 기동이 빠른 App과 사용하는 것을 권장
    * `Stateless App`에 사용한다.
    * Stateful App도 지원하지만, Stateful App은 Pod마다 역할이 있기 때문에 HPA가 어느 Pod를 늘리고 삭제할지 판단할 수 없다.
    * 그러므로 양적인 증가/감소가 가능한 `Stateless App`에 사용하는 것을 권장
  ### VPA
  * Controller에 replicas 1로 Pod가 하나 만들어져 있다.
  * 메모리는 1G, CPU는 1core로 모두 사용되는 상황
  * `VPA`를 Controller에 달아놨다면, *`VPA`는 Pod를 Restart해 Pod의 리소스를 증가시킨다.*
  * 리소스의 양이 수직적으로 증가하는 것을 `Scale UP`, 반대는 `Scale Down`<br>
  * 고려해야할 사항
    * `Stateful App`에 대한 AutoScaling할 때 사용
    * 한 Controller에 `HPA`와 `VPA`를 모두 달면 기능이 작동하지 않는다.
  ### CA
  * 클러스터에 있는 모든 Node에 자원이 없을 경우 동적으로 `worker Node`를 추가시킨다.
  * Node 2개가 있고 Pod들이 운영되고 있다.
  * 이 때, Pod를 새로 만들면 스케줄러를 통해 Node에 할당된다.
  * Worker Node의 자원이 모두 소모되고, Pod를 새로 생성하면 스케줄러는 어느 로컬 노드에도 배치하지 못하게 된다.
  * 이 때, 스케줄러는 *`CA`에 worker Node를 새로 생성해달라고 요청한다.*
  * `CA`를 사전에 특정 Cloud Provider에 연결해놨다면, 요청이 왔을 때 Provider에 Node를 만들어 스케줄러는 새로 생성한 Pod를 해당 Node에 배치한다.
  * 이렇게 운영되다가 기존에 사용하던 Pod가 삭제되어 로컬에 자원이 남을 수 있다.
  * 이 때, 스케줄러는 `CA`에게 Cloud Provider에 있는 Pod를 삭제해달라고 한다.
  * Cloud Provider에 있던 Pod는 로컬 노드로 옮겨진다.
    * Cloud Provider는 사용시간에 따라 돈을 받기 때문에 필요할 때만 사용하는 것을 권장
* ## HPA Architecture
  * Master Node
    * Master에 Control Plane Component가 있다.
    * Control Plane Component에는 쿠버네티스에서 주요 기능을 하는 컴포넌트들이 Pod 형태로 띄워져 있다. 
    * Controller Manager: Deployment, ReplicaSet, ..., HPA 등의 Controller들이 쓰레드의 형태로 돌아가고 있다.
    * kube-apiserver: 쿠버네티스의 모든 통신의 길목 역할
      * 사용자가 쿠버네티스에 접근했을 때도 이용하지만, 쿠버네티스 내의 컴포넌트들이 DB에 접근하거나 타 컴포넌트들을 호출할 때 `kube-piserver`를 통하게 된다.
  * Worker Node
    * Worker Node Component: 쿠버네티스 설치 시 kubelet이 설치된다.
    * kubelet: 각각의 Node마다 설치되고, Node를 대표하는 에이전트 역할을 해 자신의 Node에 있는 Pod를 관리한다.
    * kubelet이 직접 컨테이너를 만들진 않고, 컨테이너는 Controller Runtime이 생성하고 삭제한다.
    * Controller Runtime에는 Docker, rkt, CoreOS 등이 있다.
  * 사용자가 ReplicaSet을 만들었을 때 Pod가 생성되는 과정
    1. `ReplicaSet`을 담당하는 쓰레드는 replicas가 1일 때 Pod를 하나 만들어달라고 `kube-apiserver`를 통해 `kubelet`에 요청한다. 
    2. `kubelet`은 Pod 안의 컨테이너만 빼서 `Controller Runtime(docker)`에 만들어달라고 요청한다.
      * Pod는 쿠버네티스의 기능, 컨테이너는 Controller Runtime의 기능
    3. `Controller Runtime`은 Node 위에 컨테이너를 만들어 준다. 
  * HPA가 Pod의 성능 정보를 어떻게 아는가
    * Resource Estimator인 cAdvisor가 도커로부터 메모리나 CPU 성능정보를 측정한다.
    * 이 정보를 `kubelet`을 통해 가져갈 수 있도록 한다. <br>
  * AddOn Component
    * `AddOn Component`로 metrics-server를 설치하면 이 서버가 각각의 Node에 있는 `kubelet`으로부터 메모리와 CPU 정보를 가져와 저장한다.
    * 저장한 데이터들을 다른 컴포넌트들이 사용할 수 있도록 `kube-apiserver`에 등록해놓는다.
    * `HPA`는 CPU, 메모리 정보를 `kube-apiserver`를 통해 가져온다. 
    * `HPA`는 15초마다 정보를 확인하고, Pod의 리소스 사용률이 높아지면 replicas 수를 증가시킨다.
    * `kubectl top`명령어를 사용하면, Resource API를 통해서 Pod나 Node의 현재 리소스 상태를 조회해볼 수 있다. 
    * Prometheus를 설치하면 단순 메모리나 CPU 외에도 Pod로 들어오는 패킷 수, Ingress로 들어오는 request 양 등을 측정할 수 있다.
    * HPA는 이런 정보를 바탕으로 Controller의 replicas를 조정할 수 있다.
> 쿠버네티스 아키텍처, HPA의 동작원리, matrics-server를 추가해야되는 이유를 알아봤다.
* ## HPA
  * replicas를 2로 Deployment를 생성하면, Pod가 2개 생성된다.
  * Pod의 resources limit와 requests가 설정되어 있는 상태에서 `HPA` 생성
  * `HPA`는 target 속성으로 Controller를 지정한다.
  * maxRepicas, minRepicas로 증감되는 replicas 수의 max, minimum 지정
  * `metrics`는 metric 정보에 어떤 조건을 통해서 replicas를 증가시킬지에 대한 부분
  * `type: Resource`로 설정하면 Pod의 Resource 부분을 가리킨다.
  * 세부적으로 `name`에서 cpu를 볼지 메모리를 볼지 정할 수 있다.
  * 어떤 조건으로 증가시킬지에 대해 `type`을 지정한다.
  * type에는 기본적으로 Utilization이 있다.
    * averageUtilization이 50이면, Pod의 requests 값을 기준으로 현재 사용자원이 평균 50%가 넘으면 replicas를 증가시킨다.
    * 수치를 넘을 때마다 하나씩 증가하는것이 아니라 공식에 의해 결정<br>
  * replicas 수를 결정하는 공식
    * 리소스를 CPU라고 정했다고 가정 (name으로 cpu 지정)
    * Pod의 평균 CPU는 200m이고, averageUtilization이 50이라 할 때, 실제 CPU 사용량(target)이 100m이 넘으면 `HPA`는 replicas 수를 증가시킨다.
    * Scale Out의 예시
      > 현재 평균 CPU가 300m이라 할 때, limit cpu는 500m이므로 Pod는 죽지 않을 것이고, replicas 수에 현재 평균 CPU를 곱한 뒤 위에서 지정한 target값으로 나누면 `HPA`가 증가시킬 replicas값이 된다.
    * Scale In의 예시
      > 평균 CPU가 50m으로 떨어졌다고 할 때, Scale Out의 공식을 적용해 `HPA`가 감소시킬 replicas의 값을 구한다.<br>
    * **replicas 수 = (현재 replicas 수 x 현재 평균 CPU) / target CPU**
  * `type`의 종류에는 Utilization 외 AverageValue, Value가 있다.
    * AverageValue: 퍼센트가 아닌 원하는 성능의 실제 평균 값을 넣는다.
    * Value: 원하는 replicas의 값을 넣는다.
  * `metrics`의 type 종류
    * Pod에 Service와 Ingress가 달려있다고 할 때, Custom API를 통해
    * `type: Pods`: Pod에 들어오는 패킷 수, Pod와 관련된 정보로 Scale In/Out할 수 있다.
    * `type: object`: Pod 외에 Ingress와 같은 다른 오브젝트에 대한 정보를 사용할 때 type값
    * Custom API를 사용하려면 Prometheus와 같은 플러그인이 설치되어 있어야 한다.
 
